/**
 * üìä LATENCY TRACKING SYSTEM
 * 
 * Sistema de instrumenta√ß√£o para rastrear tempo de resposta em cada etapa do pipeline:
 * Webhook ‚Üí BullMQ ‚Üí Worker ‚Üí OpenAI ‚Üí WhatsApp
 * 
 * Meta: ‚â§30 segundos end-to-end (P95)
 */

import { redis } from './redis-config';

export interface LatencyCheckpoint {
  step: 'webhook_received' | 'queue_enqueued' | 'worker_started' | 'openai_request' | 'openai_response' | 'whatsapp_sent';
  timestamp: number; // Unix timestamp em ms
  metadata?: Record<string, any>;
}

export interface LatencyTracker {
  conversationId?: string;
  messageId: string;
  checkpoints: LatencyCheckpoint[];
  startTime: number;
}

/**
 * Cria um novo tracker de lat√™ncia
 */
export function createLatencyTracker(messageId: string, conversationId?: string): LatencyTracker {
  return {
    conversationId,
    messageId,
    checkpoints: [],
    startTime: Date.now(),
  };
}

/**
 * Adiciona um checkpoint ao tracker
 */
export function addCheckpoint(
  tracker: LatencyTracker,
  step: LatencyCheckpoint['step'],
  metadata?: Record<string, any>
): void {
  tracker.checkpoints.push({
    step,
    timestamp: Date.now(),
    metadata,
  });
}

/**
 * Calcula lat√™ncia entre dois checkpoints
 */
export function getLatencyBetween(
  tracker: LatencyTracker,
  stepA: LatencyCheckpoint['step'],
  stepB: LatencyCheckpoint['step']
): number | null {
  const checkpointA = tracker.checkpoints.find(c => c.step === stepA);
  const checkpointB = tracker.checkpoints.find(c => c.step === stepB);
  
  if (!checkpointA || !checkpointB) return null;
  
  return checkpointB.timestamp - checkpointA.timestamp;
}

/**
 * Calcula lat√™ncia total (end-to-end)
 */
export function getTotalLatency(tracker: LatencyTracker): number {
  const lastCheckpoint = tracker.checkpoints[tracker.checkpoints.length - 1];
  if (!lastCheckpoint) return 0;
  
  return lastCheckpoint.timestamp - tracker.startTime;
}

/**
 * Gera relat√≥rio de lat√™ncia com breakdown por etapa
 */
export interface LatencyReport {
  messageId: string;
  conversationId?: string;
  totalLatencyMs: number;
  breakdown: {
    webhook_to_queue?: number;
    queue_to_worker?: number;
    worker_to_openai?: number;
    openai_processing?: number;
    openai_to_whatsapp?: number;
    whatsapp_delivery?: number;
  };
  checkpoints: LatencyCheckpoint[];
  timestamp: number;
}

export function generateLatencyReport(tracker: LatencyTracker): LatencyReport {
  const breakdown: LatencyReport['breakdown'] = {};
  
  // Webhook ‚Üí Queue
  const webhookToQueue = getLatencyBetween(tracker, 'webhook_received', 'queue_enqueued');
  if (webhookToQueue !== null) breakdown.webhook_to_queue = webhookToQueue;
  
  // Queue ‚Üí Worker
  const queueToWorker = getLatencyBetween(tracker, 'queue_enqueued', 'worker_started');
  if (queueToWorker !== null) breakdown.queue_to_worker = queueToWorker;
  
  // Worker ‚Üí OpenAI Request
  const workerToOpenAI = getLatencyBetween(tracker, 'worker_started', 'openai_request');
  if (workerToOpenAI !== null) breakdown.worker_to_openai = workerToOpenAI;
  
  // OpenAI Processing (request ‚Üí response)
  const openaiProcessing = getLatencyBetween(tracker, 'openai_request', 'openai_response');
  if (openaiProcessing !== null) breakdown.openai_processing = openaiProcessing;
  
  // OpenAI Response ‚Üí WhatsApp Send
  const openaiToWhatsApp = getLatencyBetween(tracker, 'openai_response', 'whatsapp_sent');
  if (openaiToWhatsApp !== null) breakdown.openai_to_whatsapp = openaiToWhatsApp;
  
  return {
    messageId: tracker.messageId,
    conversationId: tracker.conversationId,
    totalLatencyMs: getTotalLatency(tracker),
    breakdown,
    checkpoints: tracker.checkpoints,
    timestamp: Date.now(),
  };
}

/**
 * Persiste m√©tricas de lat√™ncia no Redis (para an√°lise posterior)
 * Mant√©m √∫ltimos 1000 reports em uma lista circular
 */
export async function persistLatencyReport(report: LatencyReport): Promise<void> {
  try {
    // Adicionar ao hist√≥rico de lat√™ncia
    await redis.lpush('latency:reports', JSON.stringify(report));
    
    // Manter apenas √∫ltimos 1000 reports
    await redis.ltrim('latency:reports', 0, 999);
    
    // Atualizar m√©tricas agregadas (P50, P95, P99)
    await updateLatencyMetrics(report.totalLatencyMs);
    
    // Log estruturado
    const totalSeconds = (report.totalLatencyMs / 1000).toFixed(2);
    const openaiMs = report.breakdown.openai_processing || 0;
    const queueMs = report.breakdown.queue_to_worker || 0;
    
    console.log(`‚è±Ô∏è  [Latency] Total: ${totalSeconds}s | OpenAI: ${openaiMs}ms | Queue: ${queueMs}ms`, {
      messageId: report.messageId,
      conversationId: report.conversationId,
      breakdown: report.breakdown,
    });
    
    // Alerta se exceder 30s
    if (report.totalLatencyMs > 30000) {
      console.warn(`‚ö†Ô∏è  [Latency] SLA BREACH! Resposta demorou ${totalSeconds}s (meta: ‚â§30s)`, {
        messageId: report.messageId,
        breakdown: report.breakdown,
      });
    }
  } catch (error) {
    console.error('‚ùå [Latency] Erro ao persistir report:', error);
  }
}

/**
 * Atualiza m√©tricas agregadas de lat√™ncia (usa lista circular para percentis)
 * 
 * CRITICAL FIX v3: Usa lista circular simples (LPUSH + LTRIM)
 * - Evita desincroniza√ß√£o de dual-ZSET
 * - Opera√ß√£o at√¥mica O(1)
 * - Mant√©m √∫ltimas 1000 medi√ß√µes cronologicamente
 * - Sem bias (remove sempre as mais antigas)
 */
async function updateLatencyMetrics(latencyMs: number): Promise<void> {
  // CRITICAL: Usar lista circular ao inv√©s de ZSET para evitar bugs de cleanup
  // LPUSH + LTRIM √© at√¥mico e garante FIFO perfeito (sem bias)
  await redis.lpush('latency:measurements', latencyMs.toString());
  await redis.ltrim('latency:measurements', 0, 999); // Mant√©m √∫ltimas 1000
}

/**
 * Calcula percentis de lat√™ncia (P50, P95, P99)
 * 
 * CRITICAL FIX v3: Usa lista circular (LRANGE) para c√°lculo robusto
 * - Sem desincroniza√ß√£o
 * - Ordena√ß√£o simples em mem√≥ria
 * - Percentis matematicamente corretos
 */
export async function getLatencyPercentiles(): Promise<{
  p50: number;
  p95: number;
  p99: number;
  count: number;
} | null> {
  try {
    // Obter todas as medi√ß√µes da lista
    const measurements = await redis.lrange('latency:measurements', 0, -1);
    
    if (!Array.isArray(measurements) || measurements.length === 0) {
      return null;
    }
    
    // Converter para n√∫meros e ordenar
    const values = measurements
      .map(m => parseFloat(m as string))
      .filter(v => !isNaN(v))
      .sort((a, b) => a - b);
    
    const count = values.length;
    
    if (count === 0) {
      return null;
    }
    
    const getPercentile = (p: number) => {
      const index = Math.ceil(count * p) - 1;
      return values[Math.max(0, index)];
    };
    
    return {
      p50: getPercentile(0.50),
      p95: getPercentile(0.95),
      p99: getPercentile(0.99),
      count,
    };
  } catch (error) {
    console.error('‚ùå [Latency] Erro ao calcular percentis:', error);
    return null;
  }
}

/**
 * Obt√©m √∫ltimos N reports de lat√™ncia
 */
export async function getRecentLatencyReports(limit = 100): Promise<LatencyReport[]> {
  try {
    const reports = await redis.lrange('latency:reports', 0, limit - 1);
    
    if (!Array.isArray(reports)) {
      return [];
    }
    
    return reports.map(r => JSON.parse(r as string)) as LatencyReport[];
  } catch (error) {
    console.error('‚ùå [Latency] Erro ao obter reports:', error);
    return [];
  }
}

/**
 * Helper: Salva tracker parcial no Redis (para casos onde processo √© interrompido)
 * √ötil para recuperar checkpoints se worker crashar
 */
export async function saveTrackerSnapshot(tracker: LatencyTracker): Promise<void> {
  try {
    const key = `latency:tracker:${tracker.messageId}`;
    await redis.set(key, JSON.stringify(tracker), { ex: 300 }); // TTL 5 min
  } catch (error) {
    console.error('‚ùå [Latency] Erro ao salvar snapshot:', error);
  }
}

/**
 * Helper: Recupera tracker do Redis
 */
export async function loadTrackerSnapshot(messageId: string): Promise<LatencyTracker | null> {
  try {
    const key = `latency:tracker:${messageId}`;
    const data = await redis.get(key);
    
    if (!data) return null;
    
    return JSON.parse(data as string) as LatencyTracker;
  } catch (error) {
    console.error('‚ùå [Latency] Erro ao carregar snapshot:', error);
    return null;
  }
}
