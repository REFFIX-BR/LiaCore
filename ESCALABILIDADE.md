# ğŸ“Š Plano de Escalabilidade LIA CORTEX

**VersÃ£o**: 1.0  
**Data**: Novembro 2025  
**Autor**: Equipe TÃ©cnica LIA CORTEX  
**Status**: Em Planejamento

---

## ğŸ“‘ Ãndice

1. [Resumo Executivo](#resumo-executivo)
2. [Meta de Capacidade](#meta-de-capacidade)
3. [AnÃ¡lise da Capacidade Atual](#anÃ¡lise-da-capacidade-atual)
4. [Gargalos Identificados](#gargalos-identificados)
5. [Arquitetura de Escala Proposta](#arquitetura-de-escala-proposta)
6. [Estimativa de Custos](#estimativa-de-custos)
7. [Roadmap de ImplementaÃ§Ã£o](#roadmap-de-implementaÃ§Ã£o)
8. [Observabilidade & Monitoring](#observabilidade--monitoring)
9. [Perguntas para DecisÃ£o](#perguntas-para-decisÃ£o)
10. [PrÃ³ximos Passos](#prÃ³ximos-passos)

---

## ğŸ¯ Resumo Executivo

### Objetivo
Escalar a plataforma LIA CORTEX para suportar:
- **160.000 mensagens no pico**
- **15.000 conversas simultÃ¢neas**

### Investimentos NecessÃ¡rios

#### TÃ©cnicos
- ğŸ”´ **Workers**: 12-16 pods escalÃ¡veis (vs. 1 atual)
- ğŸ”´ **Redis**: Enterprise tier (vs. starter)
- ğŸ”´ **PostgreSQL**: Scale plan com pooling (vs. basic)
- ğŸ”´ **OpenAI**: Enterprise tier com rate limits aumentados

#### Financeiros
- ğŸ’° **Curto prazo** (Fase 1-2): +$1,500-2,000/mÃªs
- ğŸ’° **MÃ©dio prazo** (Fase 3): +$3,000-4,500/mÃªs
- ğŸ’° **Longo prazo** (Fase 4): +$5,000-8,000/mÃªs
- ğŸ’° **OpenAI** (variÃ¡vel): $7,000-10,000/mÃªs no pico

#### Timeline
- â±ï¸ **3 meses**: Capacidade para 100-120 msg/s
- â±ï¸ **6 meses**: Capacidade para 180-200 msg/s (meta alcanÃ§ada)

#### ROI Esperado
- âœ… Suporta 3-4x mais clientes
- âœ… Reduz 80% dos gargalos atuais
- âœ… Melhora experiÃªncia do usuÃ¡rio (menor latÃªncia)
- âœ… Permite crescimento sustentÃ¡vel

---

## ğŸ¯ Meta de Capacidade

### Requisitos de Performance

| MÃ©trica | Valor Atual | Meta | Multiplicador |
|---------|-------------|------|---------------|
| **Mensagens/Pico** | ~10k-20k | 160.000 | 8-16x |
| **Conversas SimultÃ¢neas** | ~1k-2k | 15.000 | 7-15x |
| **Throughput** | ~50 msg/s | 180+ msg/s | 3.6x |
| **LatÃªncia P95** | ~2-3s | <1s | -50% |

### DefiniÃ§Ãµes

- **Pico**: PerÃ­odo de maior carga (assumido: 1 hora)
- **Conversas SimultÃ¢neas**: Conversas ativas com mensagens nos Ãºltimos 10 minutos
- **Throughput**: Mensagens processadas por segundo (end-to-end)
- **LatÃªncia P95**: 95% das mensagens processadas em menos de X segundos

---

## ğŸ“ AnÃ¡lise da Capacidade Atual

### CÃ¡lculos de Throughput

#### CenÃ¡rio: 160k mensagens/hora (conservador)

```
Pico necessÃ¡rio: 160.000 Ã· 3.600 = ~44 msg/s
Capacidade recomendada (margem 100%): ~90 msg/s
Capacidade atual: ~50 jobs/s âš ï¸ INSUFICIENTE
```

#### CenÃ¡rio: 160k mensagens/dia (otimista)

```
MÃ©dia necessÃ¡ria: 160.000 Ã· 86.400 = ~1.85 msg/s
Pico esperado (5x mÃ©dia): ~9 msg/s
Capacidade atual: ~50 jobs/s âœ… SUFICIENTE
```

**âš ï¸ IMPORTANTE**: Este documento assume o cenÃ¡rio conservador (160k/hora).

---

## ğŸ”´ Gargalos Identificados

### VisÃ£o Geral

| Componente | Capacidade Atual | NecessÃ¡rio | Gap | Prioridade |
|------------|------------------|------------|-----|------------|
| **Workers (Message Processing)** | 20 concurrent (50 jobs/s) | 180+ jobs/s | -72% | ğŸ”´ CRÃTICO |
| **Redis (Upstash)** | ~1k commands/s | 4k+ commands/s | -75% | ğŸ”´ CRÃTICO |
| **PostgreSQL (Neon)** | ~50 connections | 100-150 connections | -67% | ğŸŸ¡ ALTO |
| **OpenAI API** | Limite padrÃ£o | 5-8k requests/min | Desconhecido | ğŸŸ¡ ALTO |
| **Evolution API** | NÃ£o confirmado | 60+ msg/s | Desconhecido | ğŸŸ¡ ALTO |
| **Network I/O** | ~50 Mbps | 300+ Mbps | -83% | ğŸŸ¢ MÃ‰DIO |

### Detalhamento por Componente

#### 1. Workers (BullMQ)

**Estado Atual**:
```typescript
// server/workers.ts
const concurrency = {
  messageProcessing: 20,  // 50 jobs/s max
  imageAnalysis: 8,
  npsSurvey: 8,
  inactivityFollowup: 2,
  autoClosure: 2,
}
```

**Problema**:
- 1 Ãºnica instÃ¢ncia processando todas as mensagens
- ContenÃ§Ã£o entre filas (financeiro compete com suporte)
- Sem autoscaling
- Sem redundÃ¢ncia (SPOF - Single Point of Failure)

**Impacto**:
- Queue depth cresce exponencialmente sob carga
- LatÃªncia aumenta para >5s durante picos
- Mensagens podem ser perdidas em caso de crash

---

#### 2. Redis (Upstash)

**Estado Atual**:
- Plano: Provavelmente Free ou Starter (~$30/mÃªs)
- Throughput: ~1,000 comandos/segundo
- ConexÃµes: ~50 simultÃ¢neas
- MemÃ³ria: ~256MB-1GB

**Problema**:
- Redis serÃ¡ hammered com 180+ jobs/s
- Cada job = 5-10 comandos Redis (enqueue, dequeue, lock, etc.)
- Total: ~1,000-1,500 comandos/segundo sob carga
- Risco de throttling e queue stalls

**Impacto**:
- Workers ficam bloqueados esperando Redis
- LatÃªncia de enqueue/dequeue aumenta
- Possible data loss em caso de throttling severo

---

#### 3. PostgreSQL (Neon)

**Estado Atual**:
```typescript
// ConexÃµes configuradas
max: 50,
min: 10,
```

**Problema**:
- 15k conversas simultÃ¢neas = alta contenÃ§Ã£o de locks
- Queries sem Ã­ndices adequados (N+1 queries)
- Sem particionamento de tabelas grandes
- Sem read replicas

**Impacto**:
- Slow queries aumentam (>1s)
- Deadlocks sob alta concorrÃªncia
- Database CPU spiking (>80%)

**Queries CrÃ­ticas** (necessitam otimizaÃ§Ã£o):
```sql
-- Listagem de conversas (executada 100+ vezes/min)
SELECT * FROM conversations 
WHERE status = 'active' 
ORDER BY updated_at DESC;

-- HistÃ³rico de mensagens (executada por conversa)
SELECT * FROM messages 
WHERE conversation_id = $1 
ORDER BY created_at ASC;
```

---

#### 4. OpenAI API

**Estado Atual**:
- Tier: Pay-as-you-go (rate limits padrÃ£o)
- Rate Limits estimados:
  - GPT-5: ~3,500 requests/min
  - GPT-4o: ~5,000 requests/min

**CÃ¡lculo de Demanda**:
```
15k conversas simultÃ¢neas Ã— 2-3 msgs/min/conversa = 30k-45k msgs/min
Considerando 30% de conversas ativas a qualquer momento:
= 9k-13.5k requests/min

EXCEDE O RATE LIMIT EM 2-3x âš ï¸
```

**Impacto**:
- 429 errors (rate limit exceeded)
- Exponential backoff aumenta latÃªncia
- Conversas ficam "travadas" aguardando retry

---

#### 5. Evolution API (WhatsApp Gateway)

**Estado Atual**:
- SLA nÃ£o confirmado
- Throughput desconhecido
- LatÃªncia P95 desconhecida

**Riscos**:
- Bottleneck externo fora do nosso controle
- Sem fallback ou redundÃ¢ncia
- DependÃªncia crÃ­tica (SPOF)

**AÃ§Ãµes NecessÃ¡rias**:
1. âœ… Confirmar SLA com provedor
2. âœ… Solicitar mÃºltiplas instÃ¢ncias
3. âœ… Implementar circuit breaker
4. âœ… Monitorar uptime e latÃªncia

---

## ğŸ—ï¸ Arquitetura de Escala Proposta

### VisÃ£o Geral

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CLIENTES (WhatsApp)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                    â”‚Evolutionâ”‚  (Round-robin entre 3 instÃ¢ncias)
                    â”‚   API   â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚               â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚Instance â”‚    â”‚Instance â”‚    â”‚Instance â”‚
    â”‚    1    â”‚    â”‚    2    â”‚    â”‚    3    â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚               â”‚               â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                    â”‚  Redis   â”‚  (Upstash Enterprise)
                    â”‚ Cluster  â”‚  (Queues particionadas)
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚               â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ Worker  â”‚    â”‚ Worker  â”‚    â”‚ Worker  â”‚
    â”‚  Pool   â”‚    â”‚  Pool   â”‚    â”‚  Pool   â”‚
    â”‚ (Msg)   â”‚    â”‚  (AI)   â”‚    â”‚ (Aux)   â”‚
    â”‚ 12 pods â”‚    â”‚ 4 pods  â”‚    â”‚ 3 pods  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚               â”‚               â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                    â”‚PostgreSQLâ”‚  (Neon Scale)
                    â”‚+PgBouncerâ”‚ (Connection pooling)
                    â”‚+ Replicasâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 1. Sistema de Filas (BullMQ + Redis)

#### a) Upgrade Upstash Redis

**Plano Recomendado**: Enterprise

| Feature | Free/Starter | Enterprise | DiferenÃ§a |
|---------|-------------|------------|-----------|
| **Comandos/s** | 1k | 10k+ | 10x |
| **ConexÃµes** | 50 | 500+ | 10x |
| **MemÃ³ria** | 256MB-1GB | 5GB-50GB | 5-50x |
| **ReplicaÃ§Ã£o** | âŒ | âœ… Multi-region | âœ… |
| **SLA** | Nenhum | 99.99% | âœ… |
| **Custo/mÃªs** | $0-30 | $500-800 | +$470-800 |

**Alternativa**: Self-hosted Redis Cluster (AWS ElastiCache, Google Memorystore)
- **PrÃ³s**: Controle total, custos previsÃ­veis
- **Contras**: Requer gerenciamento, complexidade operacional

---

#### b) Particionamento de Filas

**EstratÃ©gia**: Separar filas por domÃ­nio de assistente

**ImplementaÃ§Ã£o**:
```typescript
// server/queues.ts
import { Queue } from 'bullmq';

const ASSISTANTS = ['financeiro', 'comercial', 'suporte', 'ouvidoria', 'cancelamento'];

const queues = {
  messageProcessing: {} as Record<string, Queue>,
  imageAnalysis: new Queue('image-analysis', { connection: redisConfig }),
  npsSurvey: new Queue('nps-survey', { connection: redisConfig }),
  inactivity: new Queue('inactivity', { connection: redisConfig }),
};

// Criar fila dedicada por assistente
ASSISTANTS.forEach(assistant => {
  queues.messageProcessing[assistant] = new Queue(
    `msg-${assistant}`,
    { connection: redisConfig }
  );
});

// Enqueue baseado em assistantType
export function enqueueMessage(conversationId: string, assistantType: string) {
  const queue = queues.messageProcessing[assistantType];
  return queue.add('process', { conversationId }, {
    priority: getPriority(assistantType), // Financeiro = alta prioridade
    attempts: 3,
    backoff: { type: 'exponential', delay: 2000 },
  });
}
```

**BenefÃ­cios**:
- âœ… Isolamento de falhas (bug no financeiro nÃ£o afeta suporte)
- âœ… PriorizaÃ§Ã£o granular (urgÃªncias em fila separada)
- âœ… Melhor observabilidade (mÃ©tricas por domÃ­nio)
- âœ… Scaling independente (mais workers para financeiro se necessÃ¡rio)

---

### 2. Workers (Processamento)

#### Arquitetura Atual vs. Proposta

**Atual** (1 instÃ¢ncia monolÃ­tica):
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Single Worker Instance â”‚
â”‚  - 20 concurrent jobs   â”‚
â”‚  - All domains mixed    â”‚
â”‚  - No redundancy        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Proposta** (Cluster autoscaling):
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Message Pod 1      â”‚  â”‚ Message Pod 2      â”‚  â”‚ Message Pod N      â”‚
â”‚ - Financeiro       â”‚  â”‚ - Comercial        â”‚  â”‚ - Suporte          â”‚
â”‚ - 16 concurrent    â”‚  â”‚ - 16 concurrent    â”‚  â”‚ - 16 concurrent    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                       â–²                       â–²
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    Autoscaling Controller
                 (scale on queue depth + CPU)
```

---

#### a) Message Processing Pods

**ConfiguraÃ§Ã£o por Pod**:
```yaml
# kubernetes/message-worker-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lia-message-worker
spec:
  replicas: 12  # Base: 12 pods
  template:
    spec:
      containers:
      - name: worker
        image: lia-cortex:latest
        resources:
          requests:
            cpu: "2000m"      # 2 vCPU
            memory: "4Gi"     # 4GB RAM
          limits:
            cpu: "3000m"
            memory: "6Gi"
        env:
        - name: WORKER_TYPE
          value: "message-processing"
        - name: CONCURRENCY
          value: "16"
        - name: ASSISTANT_DOMAIN
          value: "{{ assistant }}"  # Injetado por pod
```

**Throughput Total**:
```
12 pods Ã— 16 concurrent Ã— ~1 msg/s = ~192 msg/s (pico teÃ³rico)
Considerando overhead (locks, retries): ~180 msg/s (real)
```

**Autoscaling**:
```yaml
# kubernetes/message-worker-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: lia-message-worker-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: lia-message-worker
  minReplicas: 8
  maxReplicas: 20
  metrics:
  - type: External
    external:
      metric:
        name: bullmq_queue_depth
        selector:
          matchLabels:
            queue: "message-processing"
      target:
        type: AverageValue
        averageValue: "50"  # Scale up se depth > 50
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

---

#### b) AI Response Pods (Tool-Heavy Threads)

**Objetivo**: Isolar conversas com mÃºltiplas tool calls (que demoram 10-30s)

**CritÃ©rios de Roteamento**:
```typescript
// Detectar threads complexas e rotear para worker dedicado
function shouldUseAIResponseWorker(conversation: Conversation): boolean {
  return (
    conversation.toolCallsCount > 5 ||  // Muitas tool calls
    conversation.hasImageAnalysis ||     // Vision API (lento)
    conversation.hasCRMIntegration       // APIs externas (latÃªncia)
  );
}
```

**ConfiguraÃ§Ã£o**:
```yaml
replicas: 4
resources:
  cpu: "3000m"    # 3 vCPU (mais CPU para I/O-bound)
  memory: "6Gi"   # 6GB RAM (mais memÃ³ria para caching)
concurrency: 8    # Menor concorrÃªncia (jobs mais pesados)
```

---

#### c) Auxiliary Pods

**Image Analysis**:
- 2 pods Ã— 2 vCPU Ã— 4GB RAM
- GPT-4o Vision API calls
- Throughput: ~20 imagens/min

**NPS Survey**:
- 1 pod Ã— 1 vCPU Ã— 2GB RAM
- Envio de NPS apÃ³s resoluÃ§Ã£o
- Throughput: ~100 surveys/hora

**Inactivity/Auto-closure**:
- 1 pod Ã— 1 vCPU Ã— 2GB RAM
- Scheduled jobs (10min, 20min intervals)
- Low priority

---

### 3. PostgreSQL (Neon)

#### a) Upgrade de Plano

**Plano Recomendado**: Scale

| Feature | Free/Starter | Scale | DiferenÃ§a |
|---------|-------------|-------|-----------|
| **Storage** | 10GB | 200GB+ (auto-scaling) | 20x+ |
| **Compute** | 0.25 vCPU | 4-8 vCPU | 16-32x |
| **ConexÃµes** | 50 | 500 | 10x |
| **TPS** | ~100 | 1,000+ | 10x+ |
| **Custo/mÃªs** | $0-50 | $300-500 | +$250-500 |

**Estimativa de Crescimento de Dados**:
```
160k mensagens/dia Ã— 365 dias = 58.4M mensagens/ano
1 mensagem â‰ˆ 1KB (mÃ©dia com metadata)
Total: ~58GB/ano de mensagens

15k conversas Ã— 30 dias avg lifecycle Ã— 100 msgs = 45M mensagens ativas
Total working set: ~45GB
```

**RecomendaÃ§Ã£o**: Provisionar 200GB iniciais, auto-scale atÃ© 500GB.

---

#### b) Connection Pooling (PgBouncer)

**Problema**: ConexÃµes diretas esgotam pool rapidamente

**SoluÃ§Ã£o**: PgBouncer como middleware

```typescript
// server/db.ts
import { Pool } from 'pg';

// SEM PgBouncer (ATUAL)
const pool = new Pool({
  connectionString: DATABASE_URL,
  max: 50,  // Esgota rÃ¡pido com 15k conversas
  min: 10,
});

// COM PgBouncer (PROPOSTO)
const pool = new Pool({
  connectionString: PGBOUNCER_URL,  // aponta para PgBouncer
  max: 100,  // PgBouncer multiplexa para Neon
  min: 20,
  idleTimeoutMillis: 10000,  // Recicla conexÃµes idle
  connectionTimeoutMillis: 2000,
});
```

**ConfiguraÃ§Ã£o PgBouncer**:
```ini
[databases]
lia_cortex = host=neon-db.com port=5432 dbname=lia_cortex

[pgbouncer]
pool_mode = transaction  # Mais eficiente para OLTP
max_client_conn = 500    # Aceita 500 workers
default_pool_size = 100  # MantÃ©m 100 conexÃµes com Neon
```

**BenefÃ­cios**:
- âœ… 5x mais conexÃµes simultÃ¢neas
- âœ… Reduz connection overhead (pool reuse)
- âœ… Transaction-level pooling (mais eficiente)

---

#### c) OtimizaÃ§Ãµes de Schema

**Ãndices Compostos CrÃ­ticos**:

```sql
-- 1. Listagem de conversas (usado 1000+ vezes/dia)
CREATE INDEX CONCURRENTLY idx_conversations_status_updated 
ON conversations (status, updated_at DESC)
INCLUDE (id, client_name, assigned_to);

-- 2. HistÃ³rico de mensagens (usado por conversa)
CREATE INDEX CONCURRENTLY idx_messages_conv_created 
ON messages (conversation_id, created_at ASC)
INCLUDE (role, content);

-- 3. Busca por cliente/documento
CREATE INDEX CONCURRENTLY idx_conversations_client_doc 
ON conversations (client_document)
WHERE client_document IS NOT NULL;

-- 4. Filtros do monitor (supervisor dashboard)
CREATE INDEX CONCURRENTLY idx_conversations_transferred_assigned 
ON conversations (transferred_to_human, assigned_to, status)
WHERE status IN ('active', 'queued');

-- 5. MÃ©tricas de performance
CREATE INDEX CONCURRENTLY idx_conversations_resolved_period 
ON conversations (resolved_at, resolved_by)
WHERE status = 'resolved';
```

**Impacto Esperado**:
- Query time: 500ms â†’ 50ms (10x mais rÃ¡pido)
- DB CPU: -40%
- Lock contention: -60%

---

#### d) Particionamento de Tabelas

**Tabela `messages`** (maior e de crescimento rÃ¡pido):

```sql
-- Converter para tabela particionada (por semana)
CREATE TABLE messages_partitioned (
  id SERIAL,
  conversation_id VARCHAR,
  role VARCHAR,
  content TEXT,
  created_at TIMESTAMP,
  -- ... outros campos
) PARTITION BY RANGE (created_at);

-- Criar partiÃ§Ãµes (Ãºltimas 8 semanas)
CREATE TABLE messages_2025_w45 PARTITION OF messages_partitioned
  FOR VALUES FROM ('2025-11-01') TO ('2025-11-08');

CREATE TABLE messages_2025_w46 PARTITION OF messages_partitioned
  FOR VALUES FROM ('2025-11-08') TO ('2025-11-15');

-- ... etc

-- PartiÃ§Ã£o "default" para dados futuros
CREATE TABLE messages_default PARTITION OF messages_partitioned
  DEFAULT;
```

**AutomaÃ§Ã£o** (cron semanal):
```sql
-- Criar nova partiÃ§Ã£o toda semana
CREATE TABLE messages_{{ next_week }} PARTITION OF messages_partitioned
  FOR VALUES FROM ('{{ start_date }}') TO ('{{ end_date }}');

-- Dropar partiÃ§Ãµes antigas (>90 dias)
DROP TABLE messages_{{ old_week }};
```

**BenefÃ­cios**:
- âœ… Queries 3-5x mais rÃ¡pidas (partition pruning)
- âœ… Vacuum/Analyze mais eficiente
- âœ… Archival simplificado (drop partition vs delete rows)

---

#### e) Archival Strategy

**Objetivo**: Mover dados antigos para cold storage

**CritÃ©rios**:
- Conversas resolvidas hÃ¡ >90 dias
- Mensagens de conversas arquivadas
- Logs de atividade antigos

**ImplementaÃ§Ã£o**:
```sql
-- 1. Exportar para S3/Object Storage
COPY (
  SELECT * FROM conversations 
  WHERE status = 'resolved' 
  AND resolved_at < NOW() - INTERVAL '90 days'
) TO PROGRAM 'aws s3 cp - s3://lia-cortex-archive/conversations/2025-q1.csv.gz --compress gzip';

-- 2. Deletar do banco ativo
DELETE FROM conversations 
WHERE status = 'resolved' 
AND resolved_at < NOW() - INTERVAL '90 days';

-- 3. Vacuum para liberar espaÃ§o
VACUUM FULL conversations;
```

**Economia Esperada**:
- Storage: -60% (apÃ³s primeiro archival)
- Query performance: +40% (working set menor)

---

### 4. OpenAI API

#### a) Upgrade para Enterprise Tier

**Rate Limits Atuais** (Pay-as-you-go):
- GPT-5: ~3,500 requests/min
- GPT-4o: ~5,000 requests/min
- Total: ~8,500 requests/min

**Rate Limits NecessÃ¡rios**:
```
15k conversas Ã— 30% ativas Ã— 3 msgs/min = 13,500 requests/min
Margem de seguranÃ§a (2x): 27,000 requests/min
```

**Enterprise Tier** (negociado):
- GPT-5: 15,000 requests/min
- GPT-4o: 15,000 requests/min
- Total: 30,000 requests/min âœ…

**Como Solicitar**:
1. Contatar OpenAI Sales: sales@openai.com
2. Informar volume esperado: 13.5k req/min
3. Solicitar rate limit increase: 30k req/min
4. Negociar pricing (desconto por volume)

**Timeline**: 2-4 semanas para aprovaÃ§Ã£o

---

#### b) OtimizaÃ§Ãµes de Custo

**1. Caching de Embeddings**:

```typescript
// server/lib/embeddings-cache.ts
import { LRUCache } from 'lru-cache';

const embeddingCache = new LRUCache<string, number[]>({
  max: 10000,  // 10k embeddings em memÃ³ria
  ttl: 1000 * 60 * 60 * 24,  // 24h
  updateAgeOnGet: true,
});

export async function getEmbeddingCached(text: string): Promise<number[]> {
  const cacheKey = hashText(text);
  
  // Check cache
  const cached = embeddingCache.get(cacheKey);
  if (cached) {
    console.log('ğŸ¯ Embedding cache HIT');
    return cached;
  }
  
  // Miss - fetch from OpenAI
  const embedding = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text,
  });
  
  const vector = embedding.data[0].embedding;
  embeddingCache.set(cacheKey, vector);
  
  return vector;
}
```

**Economia**: -35% de embedding calls (queries comuns repetidas)

---

**2. Batch de Queries RAG**:

```typescript
// ANTES (individual queries)
for (const query of userQueries) {
  const results = await vectorStore.query(query);
  // Processa cada resultado
}

// DEPOIS (batch)
const batchResults = await vectorStore.queryBatch(userQueries);
// Processa todos juntos
```

**Economia**: -50% de latÃªncia, -20% de custos (menos roundtrips)

---

**3. Modelo SeleÃ§Ã£o Inteligente**:

```typescript
// server/lib/model-selector.ts
export function selectModel(context: {
  messageLength: number;
  hasToolCalls: boolean;
  assistantType: string;
  complexity: number;
}): string {
  // Tarefas simples: gpt-4o-mini ($0.15/1M tokens)
  if (
    context.messageLength < 500 &&
    !context.hasToolCalls &&
    context.complexity < 3
  ) {
    return 'gpt-4o-mini';
  }
  
  // Tarefas mÃ©dias: gpt-4o ($2.50/1M tokens)
  if (context.complexity < 7) {
    return 'gpt-4o';
  }
  
  // Tarefas complexas: gpt-5 ($10/1M tokens)
  return 'gpt-5';
}
```

**Economia**: -30% de custos OpenAI (mix otimizado de modelos)

---

#### c) Estimativa de Custos OpenAI

**CenÃ¡rio Pico** (160k mensagens/dia):

```
Tokens por mensagem:
- Input: ~800 tokens (histÃ³rico + prompt)
- Output: ~200 tokens (resposta AI)
- Total: ~1,000 tokens/mensagem

Mix de modelos (apÃ³s otimizaÃ§Ã£o):
- gpt-4o-mini: 40% Ã— 160k = 64k msgs
- gpt-4o: 40% Ã— 160k = 64k msgs
- gpt-5: 20% Ã— 160k = 32k msgs

Custo diÃ¡rio:
- gpt-4o-mini: 64k Ã— 1k tokens Ã— $0.15/1M = $9.60
- gpt-4o: 64k Ã— 1k tokens Ã— $2.50/1M = $160
- gpt-5: 32k Ã— 1.5k tokens Ã— $10/1M = $480
TOTAL PICO: $649.60/dia

Custo mensal (pico 20% do tempo):
- MÃ©dia diÃ¡ria: $649.60 Ã— 0.2 + (baseline) Ã— 0.8
- Assumindo baseline = 30% do pico
- MÃ©dia: ~$260/dia Ã— 30 dias = $7,800/mÃªs
```

**Nota**: Custos podem variar Â±30% dependendo de:
- Complexidade real das conversas
- EficÃ¡cia do caching
- Taxa de tool calls
- Comprimento das respostas

---

### 5. Evolution API (WhatsApp Gateway)

#### a) SLA Confirmation

**QuestÃµes para Provedor**:

```
1. Qual o throughput mÃ¡ximo suportado?
   - Mensagens enviadas/segundo
   - Mensagens recebidas/segundo

2. Qual a latÃªncia P95 e P99?
   - Envio de mensagens
   - Webhook delivery

3. Qual o SLA de uptime?
   - 99%? 99.5%? 99.9%?

4. Como sÃ£o tratadas mensagens durante downtime?
   - Queueing?
   - Retry automÃ¡tico?

5. Existem mÃºltiplas instÃ¢ncias disponÃ­veis?
   - Para load balancing
   - Para redundÃ¢ncia

6. Qual o plano de disaster recovery?
   - RTO (Recovery Time Objective)
   - RPO (Recovery Point Objective)
```

**MÃ­nimo AceitÃ¡vel**:
- âœ… 60+ msg/s throughput
- âœ… <500ms P95 latency
- âœ… 99.5% uptime
- âœ… 2+ instÃ¢ncias disponÃ­veis

---

#### b) Multi-Instance Strategy

**ImplementaÃ§Ã£o**:

```typescript
// server/lib/evolution-api.ts
const EVOLUTION_INSTANCES = [
  {
    url: 'https://evolution1.trtelecom.net',
    weight: 3,  // Mais trÃ¡fego (instÃ¢ncia principal)
    healthy: true,
  },
  {
    url: 'https://evolution2.trtelecom.net',
    weight: 2,  // Backup secundÃ¡rio
    healthy: true,
  },
  {
    url: 'https://evolution3.trtelecom.net',
    weight: 1,  // Failover
    healthy: true,
  },
];

let instanceIndex = 0;

export function getNextEvolutionInstance(): string {
  // Weighted round-robin
  const totalWeight = EVOLUTION_INSTANCES
    .filter(i => i.healthy)
    .reduce((sum, i) => sum + i.weight, 0);
  
  let random = Math.random() * totalWeight;
  
  for (const instance of EVOLUTION_INSTANCES) {
    if (!instance.healthy) continue;
    
    random -= instance.weight;
    if (random <= 0) {
      return instance.url;
    }
  }
  
  // Fallback para primeira healthy
  return EVOLUTION_INSTANCES.find(i => i.healthy)?.url || EVOLUTION_INSTANCES[0].url;
}

// Health check (a cada 30s)
setInterval(async () => {
  for (const instance of EVOLUTION_INSTANCES) {
    try {
      const response = await fetch(`${instance.url}/health`, { timeout: 5000 });
      instance.healthy = response.ok;
    } catch (error) {
      instance.healthy = false;
      console.error(`âŒ Evolution instance ${instance.url} unhealthy`);
    }
  }
}, 30000);
```

---

#### c) Circuit Breaker

**Objetivo**: Prevenir cascading failures quando Evolution API estÃ¡ degradada

```typescript
// server/lib/circuit-breaker.ts
import { CircuitBreaker } from 'opossum';

const breakerOptions = {
  timeout: 10000,              // 10s timeout
  errorThresholdPercentage: 50, // Abre se >50% de erros
  resetTimeout: 30000,          // Tenta fechar apÃ³s 30s
  rollingCountTimeout: 60000,   // Janela de 60s
  volumeThreshold: 10,          // MÃ­nimo 10 requests para abrir
};

export const evolutionBreaker = new CircuitBreaker(
  async (instanceUrl: string, message: any) => {
    const response = await fetch(`${instanceUrl}/message/sendText`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(message),
    });
    
    if (!response.ok) {
      throw new Error(`Evolution API error: ${response.status}`);
    }
    
    return response.json();
  },
  breakerOptions
);

// Eventos
evolutionBreaker.on('open', () => {
  console.error('ğŸ”´ Circuit breaker OPEN - Evolution API degraded');
  // Alertar via PagerDuty
});

evolutionBreaker.on('halfOpen', () => {
  console.warn('ğŸŸ¡ Circuit breaker HALF-OPEN - Testing Evolution API');
});

evolutionBreaker.on('close', () => {
  console.log('ğŸŸ¢ Circuit breaker CLOSED - Evolution API recovered');
});

// Uso
export async function sendWhatsAppMessage(to: string, text: string) {
  const instance = getNextEvolutionInstance();
  
  try {
    return await evolutionBreaker.fire(instance, { to, text });
  } catch (error) {
    // Circuit breaker aberto ou erro
    console.error('Failed to send WhatsApp message:', error);
    
    // Enqueue para retry posterior
    await enqueueRetry({ to, text });
  }
}
```

---

## ğŸ” Observabilidade & Monitoring

### Stack de Monitoramento Proposto

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application â”‚ (LIA CORTEX)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€ Logs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Loki / CloudWatch
       â”‚
       â”œâ”€â”€â”€ Metrics â”€â”€â”€â”€â”€â”€â”€â”€â–º Prometheus
       â”‚
       â””â”€â”€â”€ Traces â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Jaeger / OpenTelemetry
                                     â”‚
                                     â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚   Grafana    â”‚ (Dashboards)
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  Alerting    â”‚ (PagerDuty/Slack)
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 1. MÃ©tricas CrÃ­ticas

#### a) Queue Health

```typescript
// server/metrics/bullmq.ts
import { register, Gauge, Histogram } from 'prom-client';

// Queue depth (por fila)
const queueDepth = new Gauge({
  name: 'bullmq_queue_depth',
  help: 'Number of jobs waiting in queue',
  labelNames: ['queue_name'],
});

// Job latency (tempo na fila)
const jobLatency = new Histogram({
  name: 'bullmq_job_latency_seconds',
  help: 'Time from job creation to processing start',
  labelNames: ['queue_name'],
  buckets: [0.1, 0.5, 1, 2, 5, 10, 30, 60],
});

// Job processing duration
const jobDuration = new Histogram({
  name: 'bullmq_job_duration_seconds',
  help: 'Time to process a job',
  labelNames: ['queue_name', 'status'],
  buckets: [0.1, 0.5, 1, 2, 5, 10, 30, 60],
});

// Failed jobs
const jobFailures = new Gauge({
  name: 'bullmq_job_failures_total',
  help: 'Number of failed jobs',
  labelNames: ['queue_name', 'error_type'],
});

// Worker utilization
const workerUtilization = new Gauge({
  name: 'bullmq_worker_utilization',
  help: 'Percentage of workers busy',
  labelNames: ['queue_name'],
});

// Coletar mÃ©tricas a cada 10s
setInterval(async () => {
  for (const [name, queue] of Object.entries(queues)) {
    const counts = await queue.getJobCounts();
    
    queueDepth.set({ queue_name: name }, counts.waiting + counts.delayed);
    jobFailures.set({ queue_name: name }, counts.failed);
  }
}, 10000);
```

---

#### b) Database Performance

```typescript
// server/metrics/database.ts
import { register, Gauge, Histogram } from 'prom-client';

// Active connections
const dbConnections = new Gauge({
  name: 'pg_connections_active',
  help: 'Number of active PostgreSQL connections',
});

// Query duration
const queryDuration = new Histogram({
  name: 'pg_query_duration_seconds',
  help: 'PostgreSQL query execution time',
  labelNames: ['query_type'],
  buckets: [0.01, 0.05, 0.1, 0.5, 1, 2, 5],
});

// Slow queries (>1s)
const slowQueries = new Gauge({
  name: 'pg_slow_queries_total',
  help: 'Number of queries taking >1s',
});

// Deadlocks
const deadlocks = new Gauge({
  name: 'pg_deadlocks_total',
  help: 'Number of deadlocks detected',
});

// Instrumentar queries
const originalQuery = pool.query.bind(pool);
pool.query = async function(...args) {
  const start = Date.now();
  
  try {
    const result = await originalQuery(...args);
    const duration = (Date.now() - start) / 1000;
    
    queryDuration.observe({ query_type: 'success' }, duration);
    
    if (duration > 1) {
      slowQueries.inc();
      console.warn(`ğŸŒ Slow query (${duration}s):`, args[0]);
    }
    
    return result;
  } catch (error) {
    queryDuration.observe({ query_type: 'error' }, (Date.now() - start) / 1000);
    throw error;
  }
};

// Coletar pg_stat_database
setInterval(async () => {
  const result = await pool.query(`
    SELECT numbackends, deadlocks 
    FROM pg_stat_database 
    WHERE datname = current_database()
  `);
  
  dbConnections.set(result.rows[0].numbackends);
  deadlocks.set(result.rows[0].deadlocks);
}, 30000);
```

---

#### c) OpenAI Metrics

```typescript
// server/metrics/openai.ts
import { register, Counter, Histogram, Gauge } from 'prom-client';

// Requests por modelo
const openaiRequests = new Counter({
  name: 'openai_requests_total',
  help: 'Number of OpenAI API requests',
  labelNames: ['model', 'status'],
});

// LatÃªncia
const openaiLatency = new Histogram({
  name: 'openai_latency_seconds',
  help: 'OpenAI API response time',
  labelNames: ['model'],
  buckets: [0.5, 1, 2, 5, 10, 20, 30],
});

// Tokens consumidos
const openaiTokens = new Counter({
  name: 'openai_tokens_total',
  help: 'Total tokens consumed',
  labelNames: ['model', 'type'], // type = prompt | completion
});

// Rate limit hits
const openaiRateLimits = new Counter({
  name: 'openai_rate_limit_hits_total',
  help: 'Number of 429 rate limit errors',
  labelNames: ['model'],
});

// Custo estimado
const openaiCost = new Counter({
  name: 'openai_cost_usd_total',
  help: 'Estimated OpenAI API cost in USD',
  labelNames: ['model'],
});

// Instrumentar chamadas OpenAI
export async function trackOpenAICall<T>(
  model: string,
  fn: () => Promise<T>
): Promise<T> {
  const start = Date.now();
  
  try {
    const result = await fn();
    const duration = (Date.now() - start) / 1000;
    
    openaiRequests.inc({ model, status: 'success' });
    openaiLatency.observe({ model }, duration);
    
    // Extrair tokens do resultado (se disponÃ­vel)
    if ('usage' in result) {
      openaiTokens.inc({ model, type: 'prompt' }, result.usage.prompt_tokens);
      openaiTokens.inc({ model, type: 'completion' }, result.usage.completion_tokens);
      
      // Calcular custo
      const cost = calculateCost(model, result.usage);
      openaiCost.inc({ model }, cost);
    }
    
    return result;
  } catch (error: any) {
    openaiRequests.inc({ model, status: 'error' });
    
    if (error.status === 429) {
      openaiRateLimits.inc({ model });
    }
    
    throw error;
  }
}
```

---

#### d) Conversation Metrics

```typescript
// server/metrics/conversations.ts
import { register, Gauge, Histogram } from 'prom-client';

// Conversas ativas
const activeConversations = new Gauge({
  name: 'conversations_active_total',
  help: 'Number of active conversations',
  labelNames: ['assistant_type', 'status'],
});

// Messages per second
const messagesPerSecond = new Gauge({
  name: 'conversations_messages_per_second',
  help: 'Current message throughput',
});

// Transfer rate (AI â†’ Human)
const transferRate = new Gauge({
  name: 'conversations_transfer_rate',
  help: 'Percentage of conversations transferred to human',
  labelNames: ['assistant_type'],
});

// Resolution time
const resolutionTime = new Histogram({
  name: 'conversations_resolution_time_seconds',
  help: 'Time from creation to resolution',
  labelNames: ['assistant_type', 'resolved_by'],
  buckets: [60, 300, 600, 1800, 3600, 7200, 86400],
});

// Atualizar a cada 30s
setInterval(async () => {
  // Active conversations
  const activeConvs = await storage.getConversations({
    status: ['active', 'queued'],
  });
  
  const byType = groupBy(activeConvs, 'assistantType');
  for (const [type, convs] of Object.entries(byType)) {
    activeConversations.set(
      { assistant_type: type, status: 'active' },
      convs.length
    );
  }
  
  // Messages/second (Ãºltima minuto)
  const msgCount = await redis.get('metrics:messages:last_minute');
  messagesPerSecond.set(parseFloat(msgCount || '0') / 60);
}, 30000);
```

---

### 2. Dashboards (Grafana)

#### Dashboard 1: Queue Health

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Queue Health Dashboard                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  Queue Depth (real-time)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Financeiro: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 250 jobs          â”‚    â”‚
â”‚  â”‚ Comercial:  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  50 jobs          â”‚    â”‚
â”‚  â”‚ Suporte:    â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 100 jobs          â”‚    â”‚
â”‚  â”‚ Image:      â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   5 jobs          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                      â”‚
â”‚  Job Latency (P95)                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         2.5s â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚    â”‚
â”‚  â”‚         2.0s â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚    â”‚
â”‚  â”‚         1.5s â”€â”€â”€â”€â”€â”€â”€â”€                        â”‚    â”‚
â”‚  â”‚         1.0s â”€â”€â”€â”€                            â”‚    â”‚
â”‚  â”‚         0.5s â”€â”€                              â”‚    â”‚
â”‚  â”‚              10:00  10:05  10:10  10:15     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                      â”‚
â”‚  Failed Jobs (last hour): 12 ğŸŸ¡                     â”‚
â”‚  Worker Utilization: 78% ğŸŸ¢                         â”‚
â”‚  Oldest Job Age: 45s ğŸŸ¢                             â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Dashboard 2: Database Performance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Database Performance Dashboard            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  Active Connections: 85/100 ğŸŸ¡                      â”‚
â”‚  CPU Usage: 45% ğŸŸ¢                                  â”‚
â”‚  Memory: 3.2GB/8GB ğŸŸ¢                               â”‚
â”‚                                                      â”‚
â”‚  Query Duration (P95)                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚        500ms â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚    â”‚
â”‚  â”‚        400ms â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”‚    â”‚
â”‚  â”‚        300ms â”€â”€â”€â”€â”€â”€                          â”‚    â”‚
â”‚  â”‚        200ms â”€â”€â”€â”€                            â”‚    â”‚
â”‚  â”‚        100ms â”€â”€                              â”‚    â”‚
â”‚  â”‚              10:00  10:05  10:10  10:15     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                      â”‚
â”‚  Slow Queries (>1s, last hour): 5 ğŸŸ¢               â”‚
â”‚  Deadlocks: 0 ğŸŸ¢                                    â”‚
â”‚  Replication Lag: 0ms ğŸŸ¢                            â”‚
â”‚                                                      â”‚
â”‚  Top 5 Slow Queries:                                â”‚
â”‚  1. SELECT * FROM messages WHERE... (1.2s) ğŸ”´      â”‚
â”‚  2. UPDATE conversations SET... (1.1s) ğŸŸ¡          â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Dashboard 3: OpenAI API

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              OpenAI API Dashboard                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  Requests/min: 3,250 / 30,000 ğŸŸ¢                    â”‚
â”‚  Latency P95: 2.1s ğŸŸ¢                               â”‚
â”‚  Rate Limit Hits: 0 ğŸŸ¢                              â”‚
â”‚                                                      â”‚
â”‚  Requests by Model (last hour)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ gpt-5:       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 45% (2.8k)         â”‚    â”‚
â”‚  â”‚ gpt-4o:      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 35% (2.1k)         â”‚    â”‚
â”‚  â”‚ gpt-4o-mini: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ 20% (1.2k)         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                      â”‚
â”‚  Token Consumption (last hour)                      â”‚
â”‚  - Input: 3.2M tokens                               â”‚
â”‚  - Output: 800k tokens                              â”‚
â”‚  - Total: 4M tokens                                 â”‚
â”‚                                                      â”‚
â”‚  Estimated Cost (today): $285.40 ğŸ’°                 â”‚
â”‚  Projected Monthly: $8,562 ğŸ’°                       â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 3. Alerting

#### Regras de Alerta

```yaml
# prometheus/alerts.yml
groups:
  - name: queue_alerts
    interval: 30s
    rules:
      - alert: HighQueueDepth
        expr: bullmq_queue_depth > 500
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High queue depth in {{ $labels.queue_name }}"
          description: "Queue {{ $labels.queue_name }} has {{ $value }} jobs waiting"
      
      - alert: CriticalQueueDepth
        expr: bullmq_queue_depth > 1000
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "CRITICAL: Queue depth in {{ $labels.queue_name }}"
          description: "Queue {{ $labels.queue_name }} has {{ $value }} jobs - possible system degradation"
      
      - alert: HighJobLatency
        expr: histogram_quantile(0.95, bullmq_job_latency_seconds) > 60
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High job latency in {{ $labels.queue_name }}"
          description: "P95 latency is {{ $value }}s (threshold: 60s)"
      
      - alert: HighJobFailureRate
        expr: rate(bullmq_job_failures_total[5m]) > 0.1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High job failure rate in {{ $labels.queue_name }}"
          description: "{{ $value }} jobs/s are failing"

  - name: database_alerts
    interval: 30s
    rules:
      - alert: HighDatabaseCPU
        expr: pg_cpu_usage > 70
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High database CPU usage"
          description: "PostgreSQL CPU at {{ $value }}%"
      
      - alert: CriticalDatabaseCPU
        expr: pg_cpu_usage > 85
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "CRITICAL: Database CPU usage"
          description: "PostgreSQL CPU at {{ $value }}% - immediate action required"
      
      - alert: ConnectionPoolExhausted
        expr: pg_connections_active / pg_connections_max > 0.9
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Database connection pool near exhaustion"
          description: "{{ $value }}% of connections in use"
      
      - alert: SlowQueries
        expr: rate(pg_slow_queries_total[5m]) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Multiple slow queries detected"
          description: "{{ $value }} queries/s taking >1s"

  - name: openai_alerts
    interval: 30s
    rules:
      - alert: OpenAIRateLimitHit
        expr: rate(openai_rate_limit_hits_total[1m]) > 0.01
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "OpenAI rate limits being hit"
          description: "{{ $value }}% of requests hitting rate limits"
      
      - alert: OpenAIHighLatency
        expr: histogram_quantile(0.95, openai_latency_seconds) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High OpenAI API latency"
          description: "P95 latency is {{ $value }}s"
      
      - alert: OpenAIHighCost
        expr: rate(openai_cost_usd_total[1h]) > 20
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High OpenAI API costs"
          description: "Burning ${{ $value }}/hour"

  - name: conversation_alerts
    interval: 60s
    rules:
      - alert: HighMessageThroughput
        expr: conversations_messages_per_second > 150
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High message throughput"
          description: "{{ $value }} msg/s - approaching capacity"
      
      - alert: HighTransferRate
        expr: conversations_transfer_rate > 0.30
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High AIâ†’Human transfer rate"
          description: "{{ $value }}% of conversations being transferred"
```

---

#### Canais de NotificaÃ§Ã£o

```yaml
# alertmanager/config.yml
global:
  resolve_timeout: 5m

route:
  group_by: ['alertname', 'severity']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  
  routes:
    # Alertas crÃ­ticos â†’ PagerDuty (24/7 on-call)
    - match:
        severity: critical
      receiver: 'pagerduty'
      continue: true
    
    # Alertas de warning â†’ Slack
    - match:
        severity: warning
      receiver: 'slack'
    
    # Alertas de custo â†’ Email (finance team)
    - match_re:
        alertname: '.*Cost.*'
      receiver: 'email-finance'

receivers:
  - name: 'default'
    webhook_configs:
      - url: 'http://localhost:9093/webhook'
  
  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: '<PAGERDUTY_SERVICE_KEY>'
        description: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.summary }}'
  
  - name: 'slack'
    slack_configs:
      - api_url: '<SLACK_WEBHOOK_URL>'
        channel: '#lia-alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ .CommonAnnotations.description }}'
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
  
  - name: 'email-finance'
    email_configs:
      - to: 'finance@trtelecom.net'
        from: 'alerts@trtelecom.net'
        smarthost: 'smtp.gmail.com:587'
        auth_username: '<EMAIL_USER>'
        auth_password: '<EMAIL_PASS>'
        subject: 'ğŸš¨ LIA CORTEX: {{ .GroupLabels.alertname }}'
```

---

## ğŸ’° Estimativa de Custos

### Custos Atuais (Baseline)

| Componente | Plano Atual | Custo/MÃªs | Notas |
|------------|-------------|-----------|-------|
| Upstash Redis | Free/Starter | $30 | 1k cmds/s |
| Neon PostgreSQL | Free/Starter | $50 | 50 connections |
| Upstash Vector | Standard | $50 | 1M vectors |
| Compute (Workers) | Replit | IncluÃ­do | Single instance |
| OpenAI API | Pay-as-you-go | $2,000 | Volume atual |
| Evolution API | ? | ? | A confirmar |
| Monitoring | Nenhum | $0 | - |
| **TOTAL** | - | **~$2,130** | - |

---

### Custos Escalados (Meta: 160k msgs/pico)

#### Fase 1-2: Vertical Scaling (MÃªs 1-2)

| Componente | Plano Novo | Custo/MÃªs | Î” Custo |
|------------|------------|-----------|---------|
| Upstash Redis | **Enterprise** | $600 | +$570 |
| Neon PostgreSQL | **Scale** | $400 | +$350 |
| Upstash Vector | **Pro** | $150 | +$100 |
| Compute | Replit | IncluÃ­do | $0 |
| OpenAI API | Pay-as-you-go | $4,000 | +$2,000 |
| Evolution API | ? | ? | ? |
| Monitoring | Prometheus/Grafana Cloud | $50 | +$50 |
| **TOTAL** | - | **~$5,200** | **+$3,070** |

**Capacidade**: ~100 msg/s

---

#### Fase 3: Horizontal Scaling (MÃªs 3-4)

| Componente | Plano Novo | Custo/MÃªs | Î” Custo (vs Fase 2) |
|------------|------------|-----------|---------------------|
| Upstash Redis | Enterprise | $600 | $0 |
| Neon PostgreSQL | Scale | $400 | $0 |
| Upstash Vector | Pro | $150 | $0 |
| **Compute** | **AWS ECS/K8s** | **$1,200** | **+$1,200** |
| OpenAI API | Enterprise tier | $7,500 | +$3,500 |
| Evolution API | ? | ? | ? |
| Monitoring | Grafana Cloud Pro | $150 | +$100 |
| **TOTAL** | - | **~$10,000** | **+$4,800** |

**Capacidade**: ~180 msg/s âœ…

**Breakdown Compute**:
```
Message Processing: 12 pods Ã— $60/pod = $720
AI Response: 4 pods Ã— $90/pod = $360
Auxiliary: 3 pods Ã— $40/pod = $120
TOTAL: $1,200/mÃªs
```

---

#### Fase 4: ResiliÃªncia (MÃªs 5-6)

| Componente | Plano Novo | Custo/MÃªs | Î” Custo (vs Fase 3) |
|------------|------------|-----------|---------------------|
| Upstash Redis | **Enterprise Multi-region** | $900 | +$300 |
| Neon PostgreSQL | **Scale + Replicas** | $700 | +$300 |
| Upstash Vector | Pro | $150 | $0 |
| Compute | AWS ECS/K8s | $1,200 | $0 |
| OpenAI API | Enterprise tier | $8,000 | +$500 |
| Evolution API | **Multi-instance** | $500 | +$500 |
| Monitoring | Grafana + PagerDuty | $300 | +$150 |
| **TOTAL** | - | **~$11,750** | **+$1,750** |

**Capacidade**: ~200 msg/s, 99.9% uptime âœ…

---

### Resumo de Custos

| Fase | MÃªs | Custo/MÃªs | Capacidade | Uptime |
|------|-----|-----------|------------|--------|
| **Baseline** | 0 | $2,130 | 50 msg/s | ~98% |
| **Fase 1-2** | 1-2 | $5,200 | 100 msg/s | ~99% |
| **Fase 3** | 3-4 | $10,000 | 180 msg/s | ~99.5% |
| **Fase 4** | 5-6 | $11,750 | 200 msg/s | ~99.9% |

**ROI**:
- **Capacidade**: +4x (50 â†’ 200 msg/s)
- **Custo**: +5.5x ($2,130 â†’ $11,750)
- **EficiÃªncia**: Custo por mensagem reduz 20% com economia de escala

---

### VariÃ¡veis de Custo OpenAI

**CenÃ¡rios**:

| CenÃ¡rio | Msgs/Dia | Tokens/Msg | Mix Modelos | Custo/MÃªs |
|---------|----------|------------|-------------|-----------|
| **Otimista** | 80k | 800 | 50% mini, 30% 4o, 20% 5 | $4,500 |
| **Realista** | 120k | 1,000 | 40% mini, 40% 4o, 20% 5 | $7,500 |
| **Pessimista** | 160k | 1,200 | 30% mini, 40% 4o, 30% 5 | $11,000 |

**Fatores que Aumentam Custo**:
- âŒ Contextos longos (histÃ³rico >10 mensagens)
- âŒ MÃºltiplas tool calls (retrials)
- âŒ Vision API (imagens pesadas)
- âŒ Embeddings sem cache

**Fatores que Reduzem Custo**:
- âœ… Caching de embeddings (-35%)
- âœ… Modelo seleÃ§Ã£o inteligente (-30%)
- âœ… CompressÃ£o de contexto (-20%)
- âœ… Batch queries RAG (-20%)

**Economia potencial**: -50% de custos OpenAI com otimizaÃ§Ãµes

---

## ğŸ›£ï¸ Roadmap de ImplementaÃ§Ã£o

### VisÃ£o Geral

```
Fase 0: PreparaÃ§Ã£o (2 semanas) â†’ $0
    â†“
Fase 1: Quick Wins (2-3 semanas) â†’ $0
    â†“
Fase 2: Vertical Scaling (3-4 semanas) â†’ +$3,070/mÃªs
    â†“
Fase 3: Horizontal Scaling (4-6 semanas) â†’ +$4,800/mÃªs
    â†“
Fase 4: ResiliÃªncia (6-8 semanas) â†’ +$1,750/mÃªs
    â†“
Total: 5-6 meses, $11,750/mÃªs
```

---

### Fase 0: PreparaÃ§Ã£o (Semanas 1-2)

#### Objetivos
- âœ… Estabelecer baseline de performance
- âœ… Instrumentar cÃ³digo com mÃ©tricas
- âœ… Configurar stack de observabilidade
- âœ… Validar SLAs de dependÃªncias externas

#### Tarefas

**Semana 1: InstrumentaÃ§Ã£o**

1. **Setup Prometheus + Grafana**
   - [ ] Deploy Prometheus server
   - [ ] Configurar scrapers para mÃ©tricas
   - [ ] Deploy Grafana
   - [ ] Criar dashboards iniciais

2. **Instrumentar BullMQ**
   - [ ] Adicionar mÃ©tricas de queue depth
   - [ ] Adicionar mÃ©tricas de job latency
   - [ ] Adicionar mÃ©tricas de failures
   - [ ] Testar coleta de mÃ©tricas

3. **Instrumentar PostgreSQL**
   - [ ] Ativar pg_stat_statements
   - [ ] Coletar mÃ©tricas de conexÃµes
   - [ ] Coletar mÃ©tricas de slow queries
   - [ ] Configurar query logging

4. **Instrumentar OpenAI**
   - [ ] Wrapper para tracking de requests
   - [ ] MÃ©tricas de tokens e custos
   - [ ] MÃ©tricas de latÃªncia
   - [ ] Rate limit monitoring

**Semana 2: Baseline e ValidaÃ§Ã£o**

5. **Load Testing**
   - [ ] Configurar k6 ou Artillery
   - [ ] Criar script de load test
   - [ ] Executar teste: 10 msg/s
   - [ ] Executar teste: 25 msg/s
   - [ ] Executar teste: 50 msg/s
   - [ ] Executar teste: 75 msg/s (falha esperada)
   - [ ] Documentar limite real

6. **ValidaÃ§Ã£o de SLAs**
   - [ ] Contatar Evolution API sobre SLA
   - [ ] Solicitar mÃ©tricas de throughput
   - [ ] Confirmar uptime garantido
   - [ ] Testar failover (se houver)

7. **DocumentaÃ§Ã£o**
   - [ ] Arquitetura atual (diagrama)
   - [ ] Fluxo de dados (sequence diagrams)
   - [ ] Dependency map
   - [ ] Runbook de incidentes

#### EntregÃ¡veis
- âœ… Grafana com 4 dashboards funcionais
- âœ… RelatÃ³rio de baseline (capacidade atual)
- âœ… SLAs validados (Evolution API)
- âœ… DocumentaÃ§Ã£o tÃ©cnica atualizada

#### Investimento
- **Custo**: $0
- **Tempo**: 2 semanas (1-2 pessoas)

---

### Fase 1: Quick Wins (Semanas 3-5)

#### Objetivos
- âœ… OtimizaÃ§Ãµes de cÃ³digo (sem custo adicional)
- âœ… Ganhar +30-40% throughput
- âœ… Reduzir latÃªncia P95 em 20%

#### Tarefas

**Semana 3: OtimizaÃ§Ãµes de Database**

1. **Ãndices Compostos**
   ```sql
   -- Ver seÃ§Ã£o "PostgreSQL > OtimizaÃ§Ãµes de Schema"
   CREATE INDEX CONCURRENTLY idx_conversations_status_updated...
   CREATE INDEX CONCURRENTLY idx_messages_conv_created...
   CREATE INDEX CONCURRENTLY idx_conversations_client_doc...
   ```
   - [ ] Criar 5 Ã­ndices crÃ­ticos
   - [ ] Validar query plans (EXPLAIN ANALYZE)
   - [ ] Medir impacto em prod (A/B test)

2. **Query Optimization**
   - [ ] Identificar top 10 slow queries
   - [ ] Reescrever com JOINs eficientes
   - [ ] Adicionar LIMIT onde apropriado
   - [ ] Usar prepared statements

3. **Connection Pooling**
   - [ ] Ajustar max connections: 50 â†’ 80
   - [ ] Ajustar idle timeout: 30s â†’ 10s
   - [ ] Implementar connection recycling

**Semana 4: OtimizaÃ§Ãµes de Workers**

4. **Ajustar ConcorrÃªncia**
   - [ ] Testar concurrency: 20 â†’ 30
   - [ ] Testar concurrency: 30 â†’ 40
   - [ ] Medir CPU e memory usage
   - [ ] Escolher configuraÃ§Ã£o Ã³tima

5. **Caching de Embeddings**
   - [ ] Implementar LRU cache (10k entries)
   - [ ] Integrar com queries RAG
   - [ ] Medir hit rate
   - [ ] Ajustar tamanho do cache

6. **Retry Policies**
   - [ ] Exponential backoff: 2s, 4s, 8s
   - [ ] Max attempts: 3
   - [ ] Dead letter queue
   - [ ] Alerting em DLQ depth

**Semana 5: OtimizaÃ§Ãµes de OpenAI**

7. **Model Selection**
   - [ ] Implementar heurÃ­stica de seleÃ§Ã£o
   - [ ] Testar com 20% do trÃ¡fego
   - [ ] Medir economia de custos
   - [ ] Rollout para 100%

8. **Batch RAG Queries**
   - [ ] Agrupar queries similares
   - [ ] Implementar batch API
   - [ ] Medir reduÃ§Ã£o de latÃªncia

9. **Idempotency**
   - [ ] Adicionar idempotency keys
   - [ ] Redis para dedup (TTL 24h)
   - [ ] Testar com duplicate messages

#### EntregÃ¡veis
- âœ… Throughput: 50 â†’ 70 msg/s (+40%)
- âœ… LatÃªncia P95: 3s â†’ 2.4s (-20%)
- âœ… Custos OpenAI: -15%

#### Investimento
- **Custo**: $0
- **Tempo**: 3 semanas (2-3 pessoas)

---

### Fase 2: Vertical Scaling (Semanas 6-9)

#### Objetivos
- âœ… Upgrade de infraestrutura gerenciada
- âœ… Ganhar +50% throughput adicional
- âœ… Preparar para horizontal scaling

#### Tarefas

**Semana 6: Upstash Redis Upgrade**

1. **MigraÃ§Ã£o para Enterprise**
   - [ ] Provisionar cluster Enterprise
   - [ ] Configurar replicaÃ§Ã£o
   - [ ] Migrar dados (zero downtime)
   - [ ] Validar performance

2. **Particionamento de Filas**
   - [ ] Separar filas por assistente
   - [ ] Atualizar workers (routing)
   - [ ] Testar isolamento
   - [ ] Rollout gradual (10%, 50%, 100%)

**Semana 7: Neon PostgreSQL Upgrade**

3. **Upgrade para Scale Plan**
   - [ ] Provisionar Scale instance
   - [ ] Configurar PgBouncer
   - [ ] Migrar dados (pg_dump/restore)
   - [ ] Cutover (maintenance window)

4. **Particionamento de Tabelas**
   - [ ] Particionar `messages` por semana
   - [ ] Criar 12 partiÃ§Ãµes (Ãºltimas 12 semanas)
   - [ ] Migrar dados histÃ³ricos
   - [ ] Configurar cron de manutenÃ§Ã£o

**Semana 8: OpenAI Enterprise**

5. **Rate Limit Increase**
   - [ ] Solicitar aumento (30k req/min)
   - [ ] Aguardar aprovaÃ§Ã£o (1-2 semanas)
   - [ ] Testar novos limites
   - [ ] Atualizar alerting

6. **Archival de Dados**
   - [ ] Implementar script de export (S3)
   - [ ] Exportar conversas >90 dias
   - [ ] Deletar do banco ativo
   - [ ] Vacuum database

**Semana 9: ValidaÃ§Ã£o**

7. **Load Testing (Fase 2)**
   - [ ] Teste: 80 msg/s (esperado: âœ…)
   - [ ] Teste: 100 msg/s (esperado: âœ…)
   - [ ] Teste: 120 msg/s (esperado: ğŸŸ¡)
   - [ ] Documentar resultados

8. **Monitoring**
   - [ ] Atualizar dashboards
   - [ ] Ajustar thresholds de alertas
   - [ ] Configurar PagerDuty
   - [ ] Testar alerting end-to-end

#### EntregÃ¡veis
- âœ… Throughput: 70 â†’ 110 msg/s (+57%)
- âœ… Uptime: 98% â†’ 99.5%
- âœ… Database: 50 â†’ 150 connections

#### Investimento
- **Custo**: +$3,070/mÃªs
- **Tempo**: 4 semanas (3-4 pessoas)

---

### Fase 3: Horizontal Scaling (Semanas 10-15)

#### Objetivos
- âœ… Deploy de mÃºltiplas instÃ¢ncias de workers
- âœ… Autoscaling baseado em carga
- âœ… Atingir meta de 180 msg/s

#### Tarefas

**Semanas 10-11: ContainerizaÃ§Ã£o**

1. **DockerizaÃ§Ã£o**
   - [ ] Criar Dockerfile otimizado
   - [ ] Multi-stage build
   - [ ] Otimizar tamanho da imagem (<500MB)
   - [ ] Testar localmente

2. **CI/CD Pipeline**
   - [ ] GitHub Actions workflow
   - [ ] Build automÃ¡tico
   - [ ] Push para registry (ECR/GCR)
   - [ ] Automated tests

3. **Kubernetes Manifests**
   - [ ] Deployments (message, AI, aux)
   - [ ] Services (ClusterIP)
   - [ ] ConfigMaps (env vars)
   - [ ] Secrets (credentials)

**Semanas 12-13: Deploy em Cluster**

4. **Provisionar Cluster K8s**
   - [ ] AWS EKS ou GCP GKE
   - [ ] 3-5 nodes (t3.large ou equivalent)
   - [ ] Configurar networking
   - [ ] Configurar storage (PVs)

5. **Deploy Workers**
   - [ ] Deploy message workers (3 pods iniciais)
   - [ ] Deploy AI workers (2 pods)
   - [ ] Deploy aux workers (1 pod)
   - [ ] Validar health checks

6. **Load Balancing**
   - [ ] Configurar ingress controller
   - [ ] SSL/TLS certificates
   - [ ] DNS records
   - [ ] Health checks

**Semanas 14-15: Autoscaling e ValidaÃ§Ã£o**

7. **Horizontal Pod Autoscaler**
   - [ ] HPA para message workers
   - [ ] HPA para AI workers
   - [ ] Testar scale-up (carga artificial)
   - [ ] Testar scale-down (idle)

8. **Circuit Breakers**
   - [ ] Implementar para Evolution API
   - [ ] Implementar para OpenAI API
   - [ ] Testar failover
   - [ ] Documentar comportamento

9. **Load Testing (Fase 3)**
   - [ ] Teste: 150 msg/s (esperado: âœ…)
   - [ ] Teste: 180 msg/s (esperado: âœ…)
   - [ ] Teste: 200 msg/s (esperado: ğŸŸ¡)
   - [ ] Stress test: 250 msg/s (esperado: âŒ)

10. **Performance Tuning**
    - [ ] Ajustar resources (CPU/memory)
    - [ ] Ajustar concurrency
    - [ ] Otimizar network latency
    - [ ] Final validation

#### EntregÃ¡veis
- âœ… Throughput: 110 â†’ 185 msg/s (+68%)
- âœ… Autoscaling funcional (8-20 pods)
- âœ… Zero downtime deployments

#### Investimento
- **Custo**: +$4,800/mÃªs (vs Fase 2)
- **Tempo**: 6 semanas (4-5 pessoas)

---

### Fase 4: ResiliÃªncia e HA (Semanas 16-22)

#### Objetivos
- âœ… Alta disponibilidade (99.9% uptime)
- âœ… Disaster recovery
- âœ… ProduÃ§Ã£o-ready

#### Tarefas

**Semanas 16-17: Multi-Region Redis**

1. **Redis Cluster (Multi-AZ)**
   - [ ] Deploy cluster em 3 AZs
   - [ ] Configurar automatic failover
   - [ ] Testar failover (kill node)
   - [ ] Validar zero data loss

2. **ReplicaÃ§Ã£o de Dados**
   - [ ] Active-passive replication
   - [ ] Lag monitoring (<100ms)
   - [ ] Backup automÃ¡tico (diÃ¡rio)

**Semanas 18-19: PostgreSQL HA**

3. **Read Replicas**
   - [ ] Provisionar 2 read replicas
   - [ ] Rotear queries read-only
   - [ ] Load balancing (pgpool)
   - [ ] Testar failover

4. **Point-in-Time Recovery**
   - [ ] Configurar WAL archiving
   - [ ] Testar restore (backup de 1h atrÃ¡s)
   - [ ] Testar restore (backup de 1 dia atrÃ¡s)
   - [ ] Documentar procedimento

**Semanas 20-21: Multi-Instance Evolution**

5. **Evolution API Load Balancing**
   - [ ] Configurar 3 instÃ¢ncias
   - [ ] Round-robin com weights
   - [ ] Health checks (30s interval)
   - [ ] Failover automÃ¡tico

6. **Disaster Recovery Plan**
   - [ ] Documentar RTO/RPO targets
   - [ ] Criar runbook de DR
   - [ ] Simular disaster (regiÃ£o down)
   - [ ] Testar recovery (<30min)

**Semana 22: Chaos Engineering**

7. **Chaos Testing**
   - [ ] Kill random pod (Chaos Monkey)
   - [ ] Network partition (Chaos Kong)
   - [ ] Database slow queries (latency injection)
   - [ ] Redis failover (kill primary)

8. **Final Validation**
   - [ ] Load test: 200 msg/s Ã— 1 hora
   - [ ] Soak test: 150 msg/s Ã— 24 horas
   - [ ] Spike test: 0 â†’ 250 msg/s Ã— 5min
   - [ ] Documentar resultados

9. **Production Readiness Review**
   - [ ] Security audit
   - [ ] Performance review
   - [ ] Disaster recovery validation
   - [ ] Documentation complete

#### EntregÃ¡veis
- âœ… Uptime: 99.5% â†’ 99.9%
- âœ… RTO: <30 minutos
- âœ… RPO: <5 minutos
- âœ… ProduÃ§Ã£o-ready âœ…

#### Investimento
- **Custo**: +$1,750/mÃªs (vs Fase 3)
- **Tempo**: 7 semanas (3-4 pessoas)

---

### Cronograma Visual

```
MÃªs 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ Fase 0 + Fase 1
       - InstrumentaÃ§Ã£o
       - Quick wins
       - Throughput: 50 â†’ 70 msg/s

MÃªs 2: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ Fase 2 (inÃ­cio)
       - Upgrade Redis
       - Upgrade PostgreSQL
       - Throughput: 70 â†’ 90 msg/s

MÃªs 3: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Fase 2 (fim) + Fase 3 (inÃ­cio)
       - OpenAI Enterprise
       - ContainerizaÃ§Ã£o
       - Throughput: 90 â†’ 120 msg/s

MÃªs 4: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆ Fase 3 (meio)
       - Deploy K8s cluster
       - Autoscaling
       - Throughput: 120 â†’ 160 msg/s

MÃªs 5: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ Fase 3 (fim) + Fase 4 (inÃ­cio)
       - Load testing
       - Multi-region Redis
       - Throughput: 160 â†’ 180 msg/s

MÃªs 6: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Fase 4 (fim)
       - PostgreSQL HA
       - Chaos testing
       - Production-ready âœ…
```

---

### Recursos NecessÃ¡rios

#### Equipe

| Papel | Fase 0-1 | Fase 2 | Fase 3 | Fase 4 |
|-------|----------|--------|--------|--------|
| **Backend Engineer** | 1-2 | 2 | 3 | 2 |
| **DevOps/SRE** | 1 | 2 | 3 | 2 |
| **Database Specialist** | 0.5 | 1 | 0.5 | 1 |
| **QA/Testing** | 0.5 | 1 | 1 | 1 |
| **Project Manager** | 0.5 | 0.5 | 0.5 | 0.5 |
| **TOTAL (FTE)** | 3.5-4.5 | 6.5 | 8 | 6.5 |

---

## â“ Perguntas para DecisÃ£o

### CrÃ­ticas (bloqueadoras)

1. **Qual a janela de tempo do "pico de 160k mensagens"?**
   - [ ] Por hora (mais conservador - assumido neste doc)
   - [ ] Por dia (mais otimista)
   - [ ] Por mÃªs (muito otimista)
   
   **Impacto**: Muda capacidade necessÃ¡ria em 10-100x

2. **Qual o budget aprovado para infraestrutura?**
   - [ ] $5,000/mÃªs (suficiente para Fase 2)
   - [ ] $10,000/mÃªs (suficiente para Fase 3)
   - [ ] $15,000/mÃªs (suficiente para Fase 4)
   - [ ] Outro: ___________
   
   **Impacto**: Define atÃ© qual fase podemos ir

3. **Quando precisa estar pronto?**
   - [ ] 1 mÃªs (impossÃ­vel)
   - [ ] 3 meses (possÃ­vel atÃ© Fase 2-3)
   - [ ] 6 meses (possÃ­vel Fase 4 completa)
   - [ ] FlexÃ­vel
   
   **Impacto**: Define priorizaÃ§Ã£o e tamanho da equipe

---

### Importantes (direcional)

4. **Qual o SLA de uptime exigido?**
   - [ ] 99% (12 horas downtime/ano) - Fase 1-2 suficiente
   - [ ] 99.5% (44 horas/ano) - Fase 3 suficiente
   - [ ] 99.9% (8.8 horas/ano) - Requer Fase 4
   
   **Impacto**: Define necessidade de HA/DR

5. **Evolution API tem mÃºltiplas instÃ¢ncias disponÃ­veis?**
   - [ ] Sim (qual SLA?)
   - [ ] NÃ£o (precisamos negociar)
   - [ ] NÃ£o sei
   
   **Impacto**: CrÃ­tico para resiliÃªncia

6. **Podemos migrar para AWS/GCP ou ficamos no Replit?**
   - [ ] Sim, podemos migrar
   - [ ] PreferÃ­vel ficar no Replit
   - [ ] FlexÃ­vel
   
   **Impacto**: Define estratÃ©gia de compute

---

### SecundÃ¡rias (otimizaÃ§Ã£o)

7. **Qual a distribuiÃ§Ã£o de carga ao longo do dia?**
   - [ ] Uniforme (24/7)
   - [ ] HorÃ¡rio comercial (8h-18h pico)
   - [ ] VariÃ¡vel (spikes imprevisÃ­veis)
   
   **Impacto**: Define estratÃ©gia de autoscaling

8. **Qual a taxa de crescimento esperada?**
   - [ ] 10% ao mÃªs
   - [ ] 20% ao mÃªs
   - [ ] 50% ao mÃªs
   - [ ] Explosivo (>100% ao mÃªs)
   
   **Impacto**: Define margem de seguranÃ§a

9. **Quais mÃ©tricas de negÃ³cio sÃ£o prioritÃ¡rias?**
   - [ ] LatÃªncia (UX)
   - [ ] Custo (ROI)
   - [ ] Uptime (SLA)
   - [ ] Todas igualmente
   
   **Impacto**: Define tradeoffs de otimizaÃ§Ã£o

---

## ğŸš€ PrÃ³ximos Passos Imediatos

### Semana 1-2: Kickoff e Setup

#### Segunda-feira
- [ ] **ReuniÃ£o de kickoff** (1h)
  - Apresentar este plano
  - Alinhar expectativas
  - Responder perguntas crÃ­ticas
  - Definir budget e timeline

- [ ] **Configurar ferramentas** (4h)
  - Provisionar Prometheus server
  - Deploy Grafana
  - Configurar GitHub Projects (tracking)

#### TerÃ§a-feira
- [ ] **InstrumentaÃ§Ã£o - BullMQ** (6h)
  - Adicionar mÃ©tricas de queue depth
  - Adicionar mÃ©tricas de latency
  - Testar coleta

#### Quarta-feira
- [ ] **InstrumentaÃ§Ã£o - PostgreSQL** (6h)
  - Ativar pg_stat_statements
  - Coletar mÃ©tricas de conexÃµes
  - Configurar slow query log

#### Quinta-feira
- [ ] **InstrumentaÃ§Ã£o - OpenAI** (6h)
  - Criar wrapper de tracking
  - MÃ©tricas de tokens e custos
  - MÃ©tricas de latÃªncia

#### Sexta-feira
- [ ] **Dashboards Grafana** (6h)
  - Dashboard: Queue Health
  - Dashboard: Database Performance
  - Dashboard: OpenAI API
  - Dashboard: Conversations

---

### Semana 3-4: Quick Wins

#### Objetivos
- âœ… Ganhar +30% throughput sem custo
- âœ… Load testing para baseline
- âœ… Validar SLAs externos

#### Prioridades
1. **Ãndices PostgreSQL** (impacto: alto, esforÃ§o: baixo)
2. **Caching de embeddings** (impacto: mÃ©dio, esforÃ§o: mÃ©dio)
3. **Ajustar concorrÃªncia workers** (impacto: alto, esforÃ§o: baixo)
4. **Load testing** (impacto: crÃ­tico, esforÃ§o: mÃ©dio)

---

### DecisÃµes CrÃ­ticas (AtÃ© Fim da Semana 2)

**Para prosseguir com Fase 2, precisamos de**:

1. âœ… **ConfirmaÃ§Ã£o de budget**: $5,000-10,000/mÃªs
2. âœ… **Timeline aprovada**: 3-6 meses
3. âœ… **SLA da Evolution API**: Confirmado pelo fornecedor
4. âœ… **DefiniÃ§Ã£o de "pico"**: Por hora, dia ou mÃªs?

**Sem essas respostas, nÃ£o podemos avanÃ§ar alÃ©m da Fase 1.**

---

## ğŸ“ Contatos e Suporte

### Equipe TÃ©cnica
- **Lead Engineer**: [Nome]
- **DevOps Lead**: [Nome]
- **Database Specialist**: [Nome]

### Fornecedores
- **OpenAI Sales**: sales@openai.com
- **Upstash Support**: support@upstash.com
- **Neon Support**: support@neon.tech
- **Evolution API**: [Contato do fornecedor]

### EscalaÃ§Ã£o
- **Urgente (P0)**: PagerDuty (24/7)
- **Alta (P1)**: Slack #lia-alerts
- **Normal (P2)**: Email team@trtelecom.net

---

## ğŸ“š ReferÃªncias

### DocumentaÃ§Ã£o TÃ©cnica
- [BullMQ Best Practices](https://docs.bullmq.io/guide/best-practices)
- [PostgreSQL Performance Tuning](https://wiki.postgresql.org/wiki/Performance_Optimization)
- [OpenAI Rate Limits](https://platform.openai.com/docs/guides/rate-limits)
- [Kubernetes Autoscaling](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)

### Ferramentas
- [Prometheus](https://prometheus.io/)
- [Grafana](https://grafana.com/)
- [k6 Load Testing](https://k6.io/)
- [PgBouncer](https://www.pgbouncer.org/)

### Arquitetura
- [12 Factor App](https://12factor.net/)
- [Microservices Patterns](https://microservices.io/patterns/)
- [Circuit Breaker Pattern](https://martinfowler.com/bliki/CircuitBreaker.html)

---

## ğŸ“„ Controle de VersÃµes

| VersÃ£o | Data | Autor | MudanÃ§as |
|--------|------|-------|----------|
| 1.0 | Nov 2025 | Equipe LIA | VersÃ£o inicial |

---

## âœ… AprovaÃ§Ãµes

| Stakeholder | Papel | Status | Data |
|-------------|-------|--------|------|
| [Nome] | CTO | â³ Pendente | - |
| [Nome] | CFO | â³ Pendente | - |
| [Nome] | Product Lead | â³ Pendente | - |

---

**Nota Final**: Este documento Ã© um plano vivo e serÃ¡ atualizado conforme o projeto evolui. RevisÃµes sÃ£o esperadas a cada fase.
